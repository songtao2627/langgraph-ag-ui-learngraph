"""
æµ‹è¯•AIæ¨¡å‹é›†æˆçš„è„šæœ¬ã€‚

è¿™ä¸ªè„šæœ¬æµ‹è¯•LangGraphå·¥ä½œæµä¸AIæ¨¡å‹çš„é›†æˆï¼ŒåŒ…æ‹¬é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ã€‚
"""

import asyncio
import sys
import os
import time
from typing import List, Dict, Any

# æ·»åŠ åº”ç”¨ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

from app.chat.workflow import ChatWorkflow
from app.chat.ai_models import MockAIModel
from app.chat.ai_providers import OpenAIModel, OllamaModel
from app.models.chat import ChatRequest


class TestAIModel(MockAIModel):
    """ç”¨äºæµ‹è¯•çš„AIæ¨¡å‹ï¼Œå¯ä»¥æ¨¡æ‹Ÿé”™è¯¯å’Œé‡è¯•ã€‚"""
    
    def __init__(self, fail_count: int = 0):
        """
        åˆå§‹åŒ–æµ‹è¯•æ¨¡å‹ã€‚
        
        Args:
            fail_count: è¿ç»­å¤±è´¥æ¬¡æ•°ï¼Œç”¨äºæµ‹è¯•é‡è¯•æœºåˆ¶
        """
        super().__init__(model_name="test-model")
        self.fail_count = fail_count
        self.call_count = 0
        self.calls: List[Dict[str, Any]] = []
    
    async def generate_response(self, messages, **kwargs):
        """æ¨¡æ‹ŸAIå“åº”ç”Ÿæˆï¼Œå¯ä»¥æ¨¡æ‹Ÿå¤±è´¥å’Œé‡è¯•ã€‚"""
        self.call_count += 1
        self.calls.append({"messages": messages, "kwargs": kwargs})
        
        # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
        await asyncio.sleep(0.1)
        
        # æ¨¡æ‹Ÿé”™è¯¯
        if self.call_count <= self.fail_count:
            raise Exception(f"æ¨¡æ‹Ÿé”™è¯¯ #{self.call_count}")
        
        # æˆåŠŸç”Ÿæˆå“åº”
        if not messages:
            return "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”"
        
        last_message = messages[-1]
        content = last_message.content if hasattr(last_message, 'content') else ""
        
        return f"æµ‹è¯•å“åº”: '{content}' (è°ƒç”¨æ¬¡æ•°: {self.call_count})"


async def test_mock_model():
    """æµ‹è¯•æ¨¡æ‹ŸAIæ¨¡å‹ã€‚"""
    print("\næµ‹è¯•æ¨¡æ‹ŸAIæ¨¡å‹...")
    
    # åˆ›å»ºå·¥ä½œæµå®ä¾‹
    model = MockAIModel()
    workflow = ChatWorkflow(ai_model=model)
    
    # æµ‹è¯•åŸºæœ¬æ¶ˆæ¯å¤„ç†
    request = ChatRequest(
        message="ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•",
        conversation_id=None
    )
    
    response = await workflow.process_message(request)
    print(f"âœ“ æ¶ˆæ¯å¤„ç†æˆåŠŸ")
    print(f"  - å“åº”: {response.response[:100]}...")
    print(f"  - ä¼šè¯ID: {response.conversation_id}")
    print(f"  - å¤„ç†æ—¶é—´: {response.processing_time}s")
    
    return True


async def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ã€‚"""
    print("\næµ‹è¯•é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶...")
    
    # åˆ›å»ºä¼šå¤±è´¥ä¸¤æ¬¡çš„æµ‹è¯•æ¨¡å‹
    model = TestAIModel(fail_count=2)
    workflow = ChatWorkflow(ai_model=model)
    
    # æµ‹è¯•æ¶ˆæ¯å¤„ç†
    request = ChatRequest(
        message="æµ‹è¯•é”™è¯¯å¤„ç†",
        conversation_id=None
    )
    
    try:
        response = await workflow.process_message(request)
        print(f"âœ“ é”™è¯¯å¤„ç†æˆåŠŸ")
        print(f"  - å“åº”: {response.response[:100]}...")
        print(f"  - è°ƒç”¨æ¬¡æ•°: {model.call_count}")
        
        if model.call_count > 1:
            print(f"âœ“ é‡è¯•æœºåˆ¶å·¥ä½œæ­£å¸¸")
        else:
            print(f"âš  é‡è¯•æœºåˆ¶å¯èƒ½æœªæ­£å¸¸å·¥ä½œ")
        
        return True
    except Exception as e:
        print(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


async def test_context_passing():
    """æµ‹è¯•ä¸Šä¸‹æ–‡ä¼ é€’ã€‚"""
    print("\næµ‹è¯•ä¸Šä¸‹æ–‡ä¼ é€’...")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = TestAIModel()
    workflow = ChatWorkflow(ai_model=model)
    
    # ç¬¬ä¸€æ¡æ¶ˆæ¯
    request1 = ChatRequest(
        message="ç¬¬ä¸€æ¡æµ‹è¯•æ¶ˆæ¯",
        conversation_id=None
    )
    
    response1 = await workflow.process_message(request1)
    conversation_id = response1.conversation_id
    
    # ç¬¬äºŒæ¡æ¶ˆæ¯
    request2 = ChatRequest(
        message="ç¬¬äºŒæ¡æµ‹è¯•æ¶ˆæ¯",
        conversation_id=conversation_id
    )
    
    response2 = await workflow.process_message(request2)
    
    # éªŒè¯ä¸Šä¸‹æ–‡ä¼ é€’
    print(f"âœ“ ä¸Šä¸‹æ–‡ä¼ é€’æµ‹è¯•")
    print(f"  - ç¬¬ä¸€æ¡å“åº”: {response1.response[:50]}...")
    print(f"  - ç¬¬äºŒæ¡å“åº”: {response2.response[:50]}...")
    
    # æ£€æŸ¥æ¨¡å‹è°ƒç”¨ä¸­çš„æ¶ˆæ¯å†å²
    if len(model.calls) >= 2:
        second_call = model.calls[1]
        messages = second_call["messages"]
        if len(messages) >= 3:  # åº”è¯¥åŒ…å«è‡³å°‘3æ¡æ¶ˆæ¯ï¼ˆç”¨æˆ·1, AI1, ç”¨æˆ·2ï¼‰
            print(f"âœ“ æ¶ˆæ¯å†å²æ­£ç¡®ä¼ é€’")
            print(f"  - å†å²æ¶ˆæ¯æ•°é‡: {len(messages)}")
        else:
            print(f"âš  æ¶ˆæ¯å†å²å¯èƒ½æœªæ­£ç¡®ä¼ é€’")
    
    return True


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•ã€‚"""
    print("å¼€å§‹æµ‹è¯•AIæ¨¡å‹é›†æˆ...")
    
    tests = [
        ("æ¨¡æ‹ŸAIæ¨¡å‹æµ‹è¯•", test_mock_model),
        ("é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶æµ‹è¯•", test_error_handling),
        ("ä¸Šä¸‹æ–‡ä¼ é€’æµ‹è¯•", test_context_passing)
    ]
    
    results = []
    
    for name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {name}")
        try:
            start_time = time.time()
            success = await test_func()
            duration = time.time() - start_time
            results.append((name, success, duration))
        except Exception as e:
            import traceback
            print(f"âŒ æµ‹è¯•å‡ºé”™: {str(e)}")
            traceback.print_exc()
            results.append((name, False, 0))
    
    # æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦
    print("\næµ‹è¯•ç»“æœæ‘˜è¦:")
    all_passed = True
    for name, success, duration in results:
        status = "âœ“ é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{status} {name} ({duration:.2f}s)")
        if not success:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼AIæ¨¡å‹é›†æˆå·¥ä½œæ­£å¸¸ã€‚")
    else:
        print("\nâš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯ã€‚")
    
    return all_passed


if __name__ == "__main__":
    success = asyncio.run(run_all_tests())
    sys.exit(0 if success else 1)