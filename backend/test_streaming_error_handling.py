#!/usr/bin/env python3
"""
æµ‹è¯•æµå¼èŠå¤©é”™è¯¯å¤„ç†åŠŸèƒ½ã€‚

è¿™ä¸ªæµ‹è¯•æ–‡ä»¶éªŒè¯æµå¼èŠå¤©ç³»ç»Ÿçš„é”™è¯¯å¤„ç†æ˜¯å¦æŒ‰ç…§è¦æ±‚3.1ã€3.2ã€3.3æ­£ç¡®å·¥ä½œã€‚
"""

import asyncio
import pytest
import logging
from unittest.mock import AsyncMock, MagicMock, patch
from typing import AsyncGenerator

from app.models.chat import ChatRequest, StreamChunk
from app.chat.workflow import ChatWorkflow
from app.chat.ai_models import AIModelInterface

# é…ç½®æµ‹è¯•æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MockFailingAIModel(AIModelInterface):
    """æ¨¡æ‹Ÿå¤±è´¥çš„AIæ¨¡å‹ï¼Œç”¨äºæµ‹è¯•é”™è¯¯å¤„ç†"""
    
    def __init__(self, error_type: str = "general"):
        self.error_type = error_type
    
    async def generate_response(self, messages, **kwargs) -> str:
        raise Exception("Mock AI model error")
    
    async def generate_response_stream(self, messages, **kwargs) -> AsyncGenerator[str, None]:
        """æ ¹æ®é”™è¯¯ç±»å‹æŠ›å‡ºä¸åŒçš„å¼‚å¸¸"""
        if self.error_type == "timeout":
            raise asyncio.TimeoutError("AI model timeout")
        elif self.error_type == "connection":
            raise ConnectionError("Cannot connect to AI service")
        elif self.error_type == "auth":
            raise ValueError("API key authentication failed")
        elif self.error_type == "rate_limit":
            raise Exception("Rate limit exceeded")
        elif self.error_type == "empty_response":
            # ä¸äº§ç”Ÿä»»ä½•å†…å®¹ï¼Œæ¨¡æ‹Ÿç©ºå“åº”
            return
            yield  # è¿™è¡Œæ°¸è¿œä¸ä¼šæ‰§è¡Œ
        else:
            raise Exception("General AI model error")
    
    async def get_model_info(self):
        return {"provider": "Mock", "model_name": "test"}


class MockEmptyAIModel(AIModelInterface):
    """æ¨¡æ‹Ÿè¿”å›ç©ºå†…å®¹çš„AIæ¨¡å‹"""
    
    async def generate_response(self, messages, **kwargs) -> str:
        return ""
    
    async def generate_response_stream(self, messages, **kwargs) -> AsyncGenerator[str, None]:
        # ä¸äº§ç”Ÿä»»ä½•å†…å®¹
        return
        yield  # è¿™è¡Œæ°¸è¿œä¸ä¼šæ‰§è¡Œ
    
    async def get_model_info(self):
        return {"provider": "MockEmpty", "model_name": "empty"}


async def test_input_validation_error():
    """æµ‹è¯•è¾“å…¥éªŒè¯é”™è¯¯å¤„ç† (è¦æ±‚3.1)"""
    workflow = ChatWorkflow()
    await workflow.async_setup()
    
    # æµ‹è¯•è¿‡é•¿æ¶ˆæ¯
    long_message = "x" * 10001
    request = ChatRequest(message="test", conversation_id="test_conv")
    # ç›´æ¥ä¿®æ”¹messageå±æ€§æ¥æµ‹è¯•éªŒè¯é€»è¾‘
    request.message = long_message
    
    chunks = []
    async for chunk in workflow.process_message_stream(request):
        chunks.append(chunk)
    
    # éªŒè¯é”™è¯¯å¤„ç†
    assert len(chunks) == 2  # start + error
    error_chunk = chunks[1]
    assert error_chunk.type == "error"
    assert "è¿‡é•¿" in error_chunk.content
    assert error_chunk.metadata["error_category"] == "input_validation"
    
    logger.info("âœ“ è¾“å…¥éªŒè¯é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")


async def test_ai_model_timeout_error():
    """æµ‹è¯•AIæ¨¡å‹è¶…æ—¶é”™è¯¯å¤„ç† (è¦æ±‚3.2)"""
    failing_model = MockFailingAIModel(error_type="timeout")
    workflow = ChatWorkflow(ai_model=failing_model)
    await workflow.async_setup()
    
    request = ChatRequest(message="æµ‹è¯•æ¶ˆæ¯", conversation_id="test_conv")
    
    chunks = []
    async for chunk in workflow.process_message_stream(request):
        chunks.append(chunk)
    
    # éªŒè¯é”™è¯¯å¤„ç†
    assert len(chunks) == 2  # start + error
    start_chunk = chunks[0]
    error_chunk = chunks[1]
    
    assert start_chunk.type == "start"
    assert error_chunk.type == "error"
    assert "è¶…æ—¶" in error_chunk.content
    assert error_chunk.metadata["error_category"] == "ai_model_timeout"
    
    logger.info("âœ“ AIæ¨¡å‹è¶…æ—¶é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")


async def test_ai_model_connection_error():
    """æµ‹è¯•AIæ¨¡å‹è¿æ¥é”™è¯¯å¤„ç† (è¦æ±‚3.2)"""
    failing_model = MockFailingAIModel(error_type="connection")
    workflow = ChatWorkflow(ai_model=failing_model)
    await workflow.async_setup()
    
    request = ChatRequest(message="æµ‹è¯•æ¶ˆæ¯", conversation_id="test_conv")
    
    chunks = []
    async for chunk in workflow.process_message_stream(request):
        chunks.append(chunk)
    
    # éªŒè¯é”™è¯¯å¤„ç†
    assert len(chunks) == 2  # start + error
    error_chunk = chunks[1]
    
    assert error_chunk.type == "error"
    assert "è¿æ¥å¤±è´¥" in error_chunk.content
    assert error_chunk.metadata["error_category"] == "network_error"
    
    logger.info("âœ“ AIæ¨¡å‹è¿æ¥é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")


async def test_ai_model_auth_error():
    """æµ‹è¯•AIæ¨¡å‹è®¤è¯é”™è¯¯å¤„ç† (è¦æ±‚3.2)"""
    failing_model = MockFailingAIModel(error_type="auth")
    workflow = ChatWorkflow(ai_model=failing_model)
    await workflow.async_setup()
    
    request = ChatRequest(message="æµ‹è¯•æ¶ˆæ¯", conversation_id="test_conv")
    
    chunks = []
    async for chunk in workflow.process_message_stream(request):
        chunks.append(chunk)
    
    # éªŒè¯é”™è¯¯å¤„ç†
    assert len(chunks) == 2  # start + error
    error_chunk = chunks[1]
    assert error_chunk.type == "error"
    assert "è®¤è¯å¤±è´¥" in error_chunk.content
    assert error_chunk.metadata["error_category"] == "authentication_error"
    
    logger.info("âœ“ AIæ¨¡å‹è®¤è¯é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")


async def test_ai_model_empty_response():
    """æµ‹è¯•AIæ¨¡å‹ç©ºå“åº”é”™è¯¯å¤„ç† (è¦æ±‚3.2)"""
    empty_model = MockEmptyAIModel()
    workflow = ChatWorkflow(ai_model=empty_model)
    await workflow.async_setup()
    
    request = ChatRequest(message="æµ‹è¯•æ¶ˆæ¯", conversation_id="test_conv")
    
    chunks = []
    async for chunk in workflow.process_message_stream(request):
        chunks.append(chunk)
    
    # éªŒè¯é”™è¯¯å¤„ç†
    error_chunk = chunks[1]
    assert error_chunk.type == "error"
    assert "æœªèƒ½ç”Ÿæˆå“åº”" in error_chunk.content
    assert error_chunk.metadata["error_category"] == "ai_model_response"
    
    logger.info("âœ“ AIæ¨¡å‹ç©ºå“åº”é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")


async def test_graceful_degradation():
    """æµ‹è¯•ä¼˜é›…é™çº§ (è¦æ±‚3.3)"""
    failing_model = MockFailingAIModel(error_type="general")
    workflow = ChatWorkflow(ai_model=failing_model)
    await workflow.async_setup()
    
    request = ChatRequest(message="æµ‹è¯•æ¶ˆæ¯", conversation_id="test_conv")
    
    chunks = []
    async for chunk in workflow.process_message_stream(request):
        chunks.append(chunk)
    
    # éªŒè¯ä¼˜é›…é™çº§
    assert len(chunks) == 2  # start + error
    start_chunk = chunks[0]
    error_chunk = chunks[1]
    
    # ç¡®ä¿å¼€å§‹äº‹ä»¶æ­£å¸¸å‘é€
    assert start_chunk.type == "start"
    assert start_chunk.metadata["status"] == "processing"
    
    # ç¡®ä¿é”™è¯¯ä¿¡æ¯ç”¨æˆ·å‹å¥½
    assert error_chunk.type == "error"
    assert "æš‚æ—¶ä¸å¯ç”¨" in error_chunk.content
    assert error_chunk.metadata["error_category"] == "ai_service_error"
    
    # ç¡®ä¿åŒ…å«å¤„ç†æ—¶é—´ç­‰å…ƒæ•°æ®
    assert "processing_time" in error_chunk.metadata
    assert error_chunk.metadata["processing_time"] > 0
    
    logger.info("âœ“ ä¼˜é›…é™çº§æµ‹è¯•é€šè¿‡")


async def test_error_chunk_format():
    """æµ‹è¯•é”™è¯¯StreamChunkæ ¼å¼ç¬¦åˆPydanticæ¨¡å‹ (è¦æ±‚4.2)"""
    failing_model = MockFailingAIModel()
    workflow = ChatWorkflow(ai_model=failing_model)
    await workflow.async_setup()
    
    request = ChatRequest(message="æµ‹è¯•æ¶ˆæ¯", conversation_id="test_conv")
    
    chunks = []
    async for chunk in workflow.process_message_stream(request):
        chunks.append(chunk)
    
    error_chunk = chunks[1]
    
    # éªŒè¯StreamChunkæ ¼å¼
    assert isinstance(error_chunk, StreamChunk)
    assert error_chunk.type == "error"
    assert isinstance(error_chunk.content, str)
    assert isinstance(error_chunk.conversation_id, str)
    assert error_chunk.timestamp is not None
    assert error_chunk.metadata is not None
    
    # éªŒè¯å…ƒæ•°æ®åŒ…å«å¿…è¦ä¿¡æ¯
    assert "error_type" in error_chunk.metadata
    assert "error_category" in error_chunk.metadata
    assert "processing_time" in error_chunk.metadata
    
    logger.info("âœ“ é”™è¯¯StreamChunkæ ¼å¼æµ‹è¯•é€šè¿‡")


async def test_client_cancellation():
    """æµ‹è¯•å®¢æˆ·ç«¯å–æ¶ˆè¯·æ±‚çš„å¤„ç†"""
    workflow = ChatWorkflow()
    await workflow.async_setup()
    
    request = ChatRequest(message="æµ‹è¯•æ¶ˆæ¯", conversation_id="test_conv")
    
    # æ¨¡æ‹Ÿå®¢æˆ·ç«¯å–æ¶ˆ
    async def cancelled_stream():
        async for chunk in workflow.process_message_stream(request):
            if chunk.type == "start":
                raise asyncio.CancelledError("Client disconnected")
            yield chunk
    
    chunks = []
    try:
        async for chunk in cancelled_stream():
            chunks.append(chunk)
    except asyncio.CancelledError:
        pass
    
    logger.info("âœ“ å®¢æˆ·ç«¯å–æ¶ˆå¤„ç†æµ‹è¯•é€šè¿‡")


async def main():
    """è¿è¡Œæ‰€æœ‰é”™è¯¯å¤„ç†æµ‹è¯•"""
    logger.info("å¼€å§‹æµå¼èŠå¤©é”™è¯¯å¤„ç†æµ‹è¯•...")
    
    tests = [
        test_input_validation_error,
        test_ai_model_timeout_error,
        test_ai_model_connection_error,
        test_ai_model_auth_error,
        test_ai_model_empty_response,
        test_graceful_degradation,
        test_error_chunk_format,
        test_client_cancellation,
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            await test()
            passed += 1
        except Exception as e:
            logger.error(f"æµ‹è¯•å¤±è´¥ {test.__name__}: {e}")
            failed += 1
    
    logger.info(f"æµ‹è¯•å®Œæˆ: {passed} é€šè¿‡, {failed} å¤±è´¥")
    
    if failed == 0:
        logger.info("ğŸ‰ æ‰€æœ‰é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        logger.error("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)