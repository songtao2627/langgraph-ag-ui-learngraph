#!/usr/bin/env python3
"""
æµ‹è¯•AIæ¨¡å‹æµå¼æ¥å£å®ç°çš„è„šæœ¬ã€‚

éªŒè¯æ‰€æœ‰AIæ¨¡å‹éƒ½æ­£ç¡®å®ç°äº†generate_response_streamæ–¹æ³•ï¼Œ
å¹¶ä¸”èƒ½å¤Ÿäº§ç”Ÿéç©ºçš„å†…å®¹å—ã€‚
"""

import asyncio
import sys
import os
from typing import List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

from langchain_core.messages import HumanMessage
from app.chat.ai_providers import MoonshotModel, OpenAIModel, OllamaModel
from app.chat.ai_models import MockAIModel


async def test_model_streaming(model, model_name: str):
    """
    æµ‹è¯•å•ä¸ªæ¨¡å‹çš„æµå¼å“åº”åŠŸèƒ½ã€‚
    
    Args:
        model: AIæ¨¡å‹å®ä¾‹
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    """
    print(f"\n=== æµ‹è¯• {model_name} æµå¼å“åº” ===")
    
    # å‡†å¤‡æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        HumanMessage(content="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚")
    ]
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰generate_response_streamæ–¹æ³•
        if not hasattr(model, 'generate_response_stream'):
            print(f"âŒ {model_name}: ç¼ºå°‘ generate_response_stream æ–¹æ³•")
            return False
        
        print(f"âœ… {model_name}: å…·æœ‰ generate_response_stream æ–¹æ³•")
        
        # æµ‹è¯•æµå¼å“åº”
        chunks_received = 0
        total_content = ""
        
        print(f"ğŸ“¡ {model_name}: å¼€å§‹æµå¼è°ƒç”¨...")
        
        async for chunk in model.generate_response_stream(test_messages):
            if chunk:  # æ£€æŸ¥chunkä¸ä¸ºç©º
                chunks_received += 1
                total_content += chunk
                print(f"ğŸ“¦ {model_name}: æ”¶åˆ°chunk #{chunks_received}: '{chunk[:50]}{'...' if len(chunk) > 50 else ''}'")
            else:
                print(f"âš ï¸  {model_name}: æ”¶åˆ°ç©ºchunk")
        
        # éªŒè¯ç»“æœ
        if chunks_received == 0:
            print(f"âŒ {model_name}: æ²¡æœ‰æ”¶åˆ°ä»»ä½•chunk")
            return False
        
        if not total_content.strip():
            print(f"âŒ {model_name}: æ‰€æœ‰chunkçš„å†…å®¹éƒ½ä¸ºç©º")
            return False
        
        print(f"âœ… {model_name}: æˆåŠŸæ”¶åˆ° {chunks_received} ä¸ªchunk")
        print(f"ğŸ“ {model_name}: æ€»å†…å®¹é•¿åº¦: {len(total_content)} å­—ç¬¦")
        print(f"ğŸ“„ {model_name}: å®Œæ•´å†…å®¹: {total_content[:100]}{'...' if len(total_content) > 100 else ''}")
        
        return True
        
    except Exception as e:
        print(f"âŒ {model_name}: æµå¼è°ƒç”¨å¤±è´¥ - {str(e)}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°ã€‚"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•AIæ¨¡å‹æµå¼æ¥å£å®ç°")
    
    # è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
    models_to_test = [
        (MockAIModel(), "MockAIModel"),
        (MoonshotModel(), "MoonshotModel"),
        (OpenAIModel(), "OpenAIModel"),
        (OllamaModel(), "OllamaModel"),
    ]
    
    results = []
    
    # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
    for model, model_name in models_to_test:
        try:
            result = await test_model_streaming(model, model_name)
            results.append((model_name, result))
        except Exception as e:
            print(f"âŒ {model_name}: æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ - {str(e)}")
            results.append((model_name, False))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*60)
    
    passed = 0
    failed = 0
    
    for model_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{model_name:20} : {status}")
        if success:
            passed += 1
        else:
            failed += 1
    
    print(f"\næ€»è®¡: {passed} ä¸ªé€šè¿‡, {failed} ä¸ªå¤±è´¥")
    
    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰æ¨¡å‹éƒ½æ­£ç¡®å®ç°äº†æµå¼æ¥å£ï¼")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æ¨¡å‹çš„æµå¼æ¥å£å®ç°æœ‰é—®é¢˜ï¼Œéœ€è¦ä¿®å¤ã€‚")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)